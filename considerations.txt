Discussion:
0 | General Considerations
1 | Total number of posts
2 | Total number of unique authors
3 | Average post length (measured in word count)
4 | Date range of the dataset
5 | Top 20 most important words in the posts
6 | Additional Tabulations


0 | General Considerations: 
    Why use Pandas?
    - Its speed and efficiency is sufficient for tabulations 1-5

1 | Total number of posts
    Assumptions:
    - [deleted] differs from [removed]
    How do we address [deleted] and [removed] posts?
    - I decided to come up with totals of all these numbers
      since the title data may still be important for analysis
      (though there are questions as to whether removed posts
      should be used at all in analysis)

3 | Average post length (measured in word count)
    Assumptions:
    - numbers count as words
    What "clean-up" was done?
    - exclude [removed] and [deleted] posts from avg post length
      calculations
    - exclude emojis and punctuation
      ex: "I like [smilelyFaceEmoji] cats ?!" has 3 words
    - reference: https://stackoverflow.com/questions/47464658/python-efficient-way-to-remove-emojis-and-some-punctuation-from-a-large-dataset

4 | Date range of the dataset
    Assumptions:
    - included [removed] and [deleted] posts in calculation assuming
      that data will be important for analysis
    Clean-up:
    - There was one line with the url in the date column. Since it was
      only one line I just ignored it. I will need to "clean up" or 
      account for data in the wrong column if I use the full_link data later

5 | Top 20 most important words in the posts
    How do we define what the most important words are? 
    - filter out filler words and do clean-up (punctuation, emojis, etc.)
    - frequency
    - key words (words like "kill" or "suicide" that are associated with high
      risk situations)
    - assign weighted values to each category (frequency, risk, etc) and then
      find words with top scores
    Limiations: 
    - Did not take into account emphasis/deemphasis with punctuation
    - Did not account for words relative to other words
    - Did not account for words relative to word requences and the post as a whole
    - Did not account for words relative to post titles
    - Should account for word stems for efficiency (go is similar to going, etc.)
    - I filtered out word variations, but I'm not sure what the implications are
      of a keeping a word like "word" vs a word like "job"
    - Not accounting for how a word is written (anything from grammar to style)

6 | Additional tabulation
  a | visualization of 5 most common topics (using title words)
      with number of comments and score
      reference for what library: https://www.analyticsvidhya.com/blog/2021/08/understanding-bar-plots-in-python-beginners-guide-to-data-visualization/
  b | ELIZA-like responses using regex matching of phrases
      References: 
      - comparing python libraries for topic modeling: 
  
7 | Ideas I didn't get to
  - in application, an automated bot that sends links to helpline phone numbers/websites as reddit replies
  - chart the emotional ranges of posts
  - Topic Modeling to find top 20 words
      - use existing Natural Language Processing algorithms to do topic modeling
        - from what I understand topic modeling has more applications to recommender
          systems and text summarization so if we defined the "most important" words
          as the words that best "summarize" or describe the dataset of posts
          then this would be a good choice
        - however, if we define "most important" words as words of high risk, 
          discrimination, isolation, and other factors (consulting more
          clinical research to determine these topics is necessary, I'm just giving
          examples) then it seems like topic modeling would not be appropriate for this
          situation
  - some sort of analysis on connections with:
    - posts containing swear words
    - posts with self-diagnosis

